= 
:allow-uri-read: 


Antes de substituir os nós originais, confirme se eles estão em um par de HA, não têm discos ausentes ou com falha, podem acessar o storage uns dos outros e não possuem LIFs de dados atribuídos aos outros nós no cluster. Você também deve coletar informações sobre os nós originais e, se o cluster estiver em um ambiente SAN, confirmar se todos os nós do cluster estão em quórum.

.Passos
. Confirme se cada um dos nós originais tem recursos suficientes para suportar adequadamente a carga de trabalho de ambos os nós durante o modo de aquisição.
+
Consulte link:other_references.html["Referências"]o link para _High Availability Management_ e siga a seção _Best Practices for HA Pairs_. Nenhum dos nós originais deve ser executado com mais de 50% de utilização; se um nó estiver executando com menos de 50% de utilização, ele poderá lidar com as cargas de ambos os nós durante a atualização da controladora.

. Conclua as subetapas a seguir para criar uma linha de base de desempenho para os nós originais:
+
.. Certifique-se de que a conta de utilizador de diagnóstico está desbloqueada.
+
[IMPORTANT]
====
A conta de utilizador de diagnóstico destina-se apenas a fins de diagnóstico de baixo nível e deve ser utilizada apenas com orientação do suporte técnico.

Para obter informações sobre como desbloquear as contas de usuário, link:other_references.html["Referências"]consulte o link para a _Referência de administração do sistema_.

====
.. Consulte o link:other_references.html["Referências"]link para o _Site de suporte NetApp_ e faça o download do Coletor de desempenho e estatísticas (Perfstat Converged).
+
A ferramenta Convergente Perfstat permite estabelecer uma linha de base de desempenho para comparação após a atualização.

.. Crie uma linha de base de desempenho, seguindo as instruções no site de suporte da NetApp.


. link:other_references.html["Referências"]Consulte o link para o site de suporte _NetApp_ e abra um caso de suporte no site de suporte da NetApp.
+
Você pode usar o caso para relatar quaisquer problemas que possam surgir durante a atualização.

. Verifique se as baterias NVMEM ou NVRAM de node3 e node4 estão carregadas e carregue-as se não estiverem.
+
Você deve verificar fisicamente node3 e node4 para ver se as baterias NVMEM ou NVRAM estão carregadas. Para obter informações sobre os LEDs para o modelo de node3 e node4, consulte a link:other_references.html["Referências"]ligação ao _Hardware Universe_.

+

WARNING: *Atenção* não tente limpar o conteúdo do NVRAM. Se houver necessidade de limpar o conteúdo do NVRAM, entre em Contato com o suporte técnico da NetApp.

. Verifique a versão do ONTAP em node3 e node4.
+
Os novos nós devem ter a mesma versão do ONTAP 9.x instalada neles que está instalada nos nós originais. Se os novos nós tiverem uma versão diferente do ONTAP instalada, você deverá inicializar os novos controladores depois de instalá-los. Para obter instruções sobre como atualizar o ONTAP, consulte link:other_references.html["Referências"]o link para _Atualizar ONTAP_.

+
As informações sobre a versão do ONTAP em node3 e node4 devem ser incluídas nas caixas de envio. A versão do ONTAP é exibida quando o nó é inicializado ou você pode inicializar o nó no modo de manutenção e executar o comando:

+
`version`

. Verifique se você tem duas ou quatro LIFs de cluster em node1 e node2:
+
`network interface show -role cluster`

+
O sistema exibe quaisquer LIFs de cluster, como mostrado no exemplo a seguir:

+
....
cluster::> network interface show -role cluster
        Logical    Status     Network          Current  Current Is
Vserver Interface  Admin/Oper Address/Mask     Node     Port    Home
------- ---------- ---------- ---------------- -------- ------- ----
node1
        clus1      up/up      172.17.177.2/24  node1    e0c     true
        clus2      up/up      172.17.177.6/24  node1    e0e     true
node2
        clus1      up/up      172.17.177.3/24  node2    e0c     true
        clus2      up/up      172.17.177.7/24  node2    e0e     true
....
. Se você tiver duas ou quatro LIFs de cluster no node1 ou node2, certifique-se de que você pode fazer o ping de ambas as LIFs de cluster em todos os caminhos disponíveis, executando as seguintes subetapas:
+
.. Introduza o nível de privilégio avançado:
+
`set -privilege advanced`

+
O sistema exibe a seguinte mensagem:

+
....
Warning: These advanced commands are potentially dangerous; use them only when directed to do so by NetApp personnel.
Do you wish to continue? (y or n):
....
.. Introduza `y`.
.. Faça ping nos nós e teste a conetividade:
+
`cluster ping-cluster -node node_name`

+
O sistema exibe uma mensagem semelhante ao seguinte exemplo:

+
....
cluster::*> cluster ping-cluster -node node1
Host is node1
Getting addresses from network interface table...
Local = 10.254.231.102 10.254.91.42
Remote = 10.254.42.25 10.254.16.228
Ping status:
...
Basic connectivity succeeds on 4 path(s) Basic connectivity fails on 0 path(s)
................
Detected 1500 byte MTU on 4 path(s):
Local 10.254.231.102 to Remote 10.254.16.228
Local 10.254.231.102 to Remote 10.254.42.25
Local 10.254.91.42 to Remote 10.254.16.228
Local 10.254.91.42 to Remote 10.254.42.25
Larger than PMTU communication succeeds on 4 path(s)
RPC status:
2 paths up, 0 paths down (tcp check)
2 paths up, 0 paths down (udp check)
....
+
Se o nó usar duas portas de cluster, você verá que ele é capaz de se comunicar em quatro caminhos, como mostrado no exemplo.

.. Retornar ao privilégio de nível administrativo:
+
`set -privilege admin`



. Confirme se o node1 e o node2 estão em um par de HA e verifique se os nós estão conetados uns aos outros e se o takeover é possível:
+
`storage failover show`

+
O exemplo a seguir mostra a saída quando os nós estão conetados uns aos outros e a aquisição é possível:

+
....
cluster::> storage failover show
                              Takeover
Node           Partner        Possible State Description
-------------- -------------- -------- -------------------------------
node1          node2          true     Connected to node2
node2          node1          true     Connected to node1
....
+
Nenhum dos nós deve estar em giveback parcial. O exemplo a seguir mostra que node1 está em parcial giveback:

+
....
cluster::> storage failover show
                              Takeover
Node           Partner        Possible State Description
-------------- -------------- -------- -------------------------------
node1          node2          true     Connected to node2, Partial giveback
node2          node1          true     Connected to node1
....
+
Se qualquer nó estiver em parcial giveback, use o `storage failover giveback` comando para executar o giveback e use o `storage failover show-giveback` comando para garantir que nenhum agregado ainda precise ser devolvido. Para obter informações detalhadas sobre os comandos, consulte link:other_references.html["Referências"]o link para _High Availability Management_.

. [[man_prepare_nodes_step9]]Confirme que nem o node1 nem o node2 possuem os agregados para os quais é o proprietário atual (mas não o proprietário da casa):
+
`storage aggregate show -nodes _node_name_ -is-home false -fields owner-name, home-name, state`

+
Se nem node1 nem node2 possuírem agregados para os quais é o proprietário atual (mas não o proprietário da casa), o sistema retornará uma mensagem semelhante ao seguinte exemplo:

+
....
cluster::> storage aggregate show -node node2 -is-home false -fields owner-name,homename,state
There are no entries matching your query.
....
+
O exemplo a seguir mostra a saída do comando para um nó chamado node2 que é o proprietário da casa, mas não o proprietário atual, de quatro agregados:

+
....
cluster::> storage aggregate show -node node2 -is-home false
               -fields owner-name,home-name,state

aggregate     home-name    owner-name   state
------------- ------------ ------------ ------
aggr1         node1        node2        online
aggr2         node1        node2        online
aggr3         node1        node2        online
aggr4         node1        node2        online

4 entries were displayed.
....
. Execute uma das seguintes ações:
+
[cols="35,65"]
|===
| Se o comando <<man_prepare_nodes_step9,Passo 9>>em ... | Então... 


| Tinha saída em branco | Pule a Etapa 11 e vá para <<man_prepare_nodes_step12,Passo 12>>. 


| Tinha saída | Vá para <<man_prepare_nodes_step11,Passo 11>>. 
|===
. [[man_prepare_nodes_step11]] se node1 ou node2 possuir agregados para os quais é o proprietário atual, mas não o proprietário da casa, complete os seguintes subpassos:
+
.. Devolva os agregados atualmente pertencentes ao nó do parceiro para o nó do proprietário da casa:
+
`storage failover giveback -ofnode _home_node_name_`

.. Verifique se nem o node1 nem o node2 ainda possuem agregados para os quais é o proprietário atual (mas não o proprietário da casa):
+
`storage aggregate show -nodes _node_name_ -is-home false -fields owner-name, home-name, state`

+
O exemplo a seguir mostra a saída do comando quando um nó é o proprietário atual e proprietário de agregados:

+
....
cluster::> storage aggregate show -nodes node1
          -is-home true -fields owner-name,home-name,state

aggregate     home-name    owner-name   state
------------- ------------ ------------ ------
aggr1         node1        node1        online
aggr2         node1        node1        online
aggr3         node1        node1        online
aggr4         node1        node1        online

4 entries were displayed.
....


. [[man_prepare_nodes_step12]] confirmar que o node1 e o node2 podem acessar o armazenamento um do outro e verificar se não há discos ausentes:
+
`storage failover show -fields local-missing-disks,partner-missing-disks`

+
O exemplo a seguir mostra a saída quando nenhum disco está faltando:

+
....
cluster::> storage failover show -fields local-missing-disks,partner-missing-disks

node     local-missing-disks partner-missing-disks
-------- ------------------- ---------------------
node1    None                None
node2    None                None
....
+
Se algum disco estiver faltando, link:other_references.html["Referências"]consulte o link para _Gerenciamento de disco e agregado com a CLI_, _Gerenciamento de armazenamento lógico com a CLI_ e _Gerenciamento de alta disponibilidade_ para configurar o armazenamento para o par de HA.

. Confirme se node1 e node2 estão saudáveis e qualificados para participar do cluster:
+
`cluster show`

+
O exemplo a seguir mostra a saída quando ambos os nós são elegíveis e íntegros:

+
....
cluster::> cluster show

Node                  Health  Eligibility
--------------------- ------- ------------
node1                 true    true
node2                 true    true
....
. Defina o nível de privilégio como avançado:
+
`set -privilege advanced`

. [[man_prepare_nodes_step15]] confirme que node1 e node2 estão executando a mesma versão do ONTAP:
+
`system node image show -node _node1,node2_ -iscurrent true`

+
O exemplo a seguir mostra a saída do comando:

+
....
cluster::*> system node image show -node node1,node2 -iscurrent true

                 Is      Is                Install
Node     Image   Default Current Version   Date
-------- ------- ------- ------- --------- -------------------
node1
         image1  true    true    9.1         2/7/2017 20:22:06
node2
         image1  true    true    9.1         2/7/2017 20:20:48

2 entries were displayed.
....
. Verifique se nem o node1 nem o node2 possuem LIFs de dados que pertencem a outros nós no cluster e verifique as `Current Node` colunas e `Is Home` na saída:
+
`network interface show -role data -is-home false -curr-node _node_name_`

+
O exemplo a seguir mostra a saída quando node1 não tem LIFs que são de propriedade própria por outros nós no cluster:

+
....
cluster::> network interface show -role data -is-home false -curr-node node1
 There are no entries matching your query.
....
+
O exemplo a seguir mostra a saída quando o node1 possui LIFs de dados de propriedade do outro nó:

+
....
cluster::> network interface show -role data -is-home false -curr-node node1

            Logical    Status     Network            Current       Current Is
Vserver     Interface  Admin/Oper Address/Mask       Node          Port    Home
----------- ---------- ---------- ------------------ ------------- ------- ----
vs0
            data1      up/up      172.18.103.137/24  node1         e0d     false
            data2      up/up      172.18.103.143/24  node1         e0f     false

2 entries were displayed.
....
. Se a saída em <<man_prepare_nodes_step15,Passo 15>> mostrar que node1 ou node2 possui quaisquer LIFs de dados de propriedade de outros nós no cluster, migre os LIFs de dados de node1 ou node2:
+
`network interface revert -vserver * -lif *`

+
Para obter informações detalhadas sobre o `network interface revert` comando, link:other_references.html["Referências"]consulte a ligação para os comandos _ONTAP 9: Manual Page Reference_.

. Verifique se o node1 ou o node2 possui quaisquer discos com falha:
+
`storage disk show -nodelist _node1,node2_ -broken`

+
Se algum dos discos tiver falhado, remova-os seguindo as instruções no _Disk e no gerenciamento de agregados com a CLI_. (Consulte a link:other_references.html["Referências"]ligação ao _Disk e ao gerenciamento de agregados com a CLI_.)

. Colete informações sobre node1 e node2, completando as seguintes subetapas e gravando a saída de cada comando:

